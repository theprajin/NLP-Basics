{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ussBZdK_MqIo",
    "outputId": "05d0a948-e5ef-4488-b92c-0e85ded54626"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() #to download required contents from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_xy_iijoM5fw"
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"A paragraph is a group of words put together to form a group that is usually longer than a sentence. Paragraphs are often made up of several sentences. There are usually between three and eight sentences. Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again. This makes it easier to see when one paragraph ends and another begins.\n",
    "\n",
    "In most organized forms of writing, such as essays, paragraphs contain a topic sentence . This topic sentence of the paragraph tells the reader what the paragraph will be about. Essays usually have multiple paragraphs that make claims to support a thesis statement, which is the central idea of the essay.\n",
    "\n",
    "Paragraphs may signal when the writer changes topics. Each paragraph may have a number of sentences, depending on the topic.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzOdmUjwjueu"
   },
   "source": [
    "**Sentence Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hq1cdENdOQ0r"
   },
   "outputs": [],
   "source": [
    "#sentence tokenizer\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2u_isOBkOeCY",
    "outputId": "3b47b998-78d8-4506-9dad-b08715dc25da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph is a group of words put together to form a group that is usually longer than a sentence.', 'Paragraphs are often made up of several sentences.', 'There are usually between three and eight sentences.', 'Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again.', 'This makes it easier to see when one paragraph ends and another begins.', 'In most organized forms of writing, such as essays, paragraphs contain a topic sentence .', 'This topic sentence of the paragraph tells the reader what the paragraph will be about.', 'Essays usually have multiple paragraphs that make claims to support a thesis statement, which is the central idea of the essay.', 'Paragraphs may signal when the writer changes topics.', 'Each paragraph may have a number of sentences, depending on the topic.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97whp8Idj1Qk"
   },
   "source": [
    "**Word Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l7uPQoO4Of3H"
   },
   "outputs": [],
   "source": [
    "#word tokenizer\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-d9SFCcGOr6i",
    "outputId": "8d155240-f9b2-4945-9b4a-ac4bfb927d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'paragraph', 'is', 'a', 'group', 'of', 'words', 'put', 'together', 'to', 'form', 'a', 'group', 'that', 'is', 'usually', 'longer', 'than', 'a', 'sentence', '.', 'Paragraphs', 'are', 'often', 'made', 'up', 'of', 'several', 'sentences', '.', 'There', 'are', 'usually', 'between', 'three', 'and', 'eight', 'sentences', '.', 'Paragraphs', 'can', 'begin', 'with', 'an', 'indentation', '(', 'about', 'five', 'spaces', ')', ',', 'or', 'by', 'missing', 'a', 'line', 'out', ',', 'and', 'then', 'starting', 'again', '.', 'This', 'makes', 'it', 'easier', 'to', 'see', 'when', 'one', 'paragraph', 'ends', 'and', 'another', 'begins', '.', 'In', 'most', 'organized', 'forms', 'of', 'writing', ',', 'such', 'as', 'essays', ',', 'paragraphs', 'contain', 'a', 'topic', 'sentence', '.', 'This', 'topic', 'sentence', 'of', 'the', 'paragraph', 'tells', 'the', 'reader', 'what', 'the', 'paragraph', 'will', 'be', 'about', '.', 'Essays', 'usually', 'have', 'multiple', 'paragraphs', 'that', 'make', 'claims', 'to', 'support', 'a', 'thesis', 'statement', ',', 'which', 'is', 'the', 'central', 'idea', 'of', 'the', 'essay', '.', 'Paragraphs', 'may', 'signal', 'when', 'the', 'writer', 'changes', 'topics', '.', 'Each', 'paragraph', 'may', 'have', 'a', 'number', 'of', 'sentences', ',', 'depending', 'on', 'the', 'topic', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y4-98e7j6Af"
   },
   "source": [
    "**Stop Words in English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "l7lXGToXOtPn"
   },
   "outputs": [],
   "source": [
    "#stopwords in english\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TmbLXkbPj7y",
    "outputId": "8c024895-a9aa-4650-ede2-cbaee1c760d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pboPTddj99y"
   },
   "source": [
    "**Removing Stop-Words in English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0ggLx7GcE2k",
    "outputId": "6538da21-1cfa-4fc2-8f6b-ab5b2489be37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph is a group of words put together to form a group that is usually longer than a sentence.', 'Paragraphs often made several sentences .', 'There are usually between three and eight sentences.', 'Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again.', 'This makes it easier to see when one paragraph ends and another begins.', 'In most organized forms of writing, such as essays, paragraphs contain a topic sentence .', 'This topic sentence of the paragraph tells the reader what the paragraph will be about.', 'Essays usually have multiple paragraphs that make claims to support a thesis statement, which is the central idea of the essay.', 'Paragraphs may signal when the writer changes topics.', 'Each paragraph may have a number of sentences, depending on the topic.']\n"
     ]
    }
   ],
   "source": [
    "#removing stop words in english\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "  words = nltk.word_tokenize(sentences[1])\n",
    "  words = [word for word in words if word not in set(stopwords.words('english'))]\n",
    "  sentences[1] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4sFKfc0kIN1"
   },
   "source": [
    "**Stemming in NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsW8MEexP9j0",
    "outputId": "ade1a361-d7ec-49c0-a8a1-9bc69fb0c718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph group word put togeth form group usual longer sentenc .', 'paragraph often made sever sentenc .', 'there usual three eight sentenc .', 'paragraph begin indent ( five space ) , miss line , start .', 'thi make easier see one paragraph end anoth begin .', 'In organ form write , essay , paragraph contain topic sentenc .', 'thi topic sentenc paragraph tell reader paragraph .', 'essay usual multipl paragraph make claim support thesi statement , central idea essay .', 'paragraph may signal writer chang topic .', 'each paragraph may number sentenc , depend topic .']\n"
     ]
    }
   ],
   "source": [
    "#stemming in NLP\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "  words = nltk.word_tokenize(sentences[i])\n",
    "  words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "  sentences[i] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX_Op42YkLQ2"
   },
   "source": [
    "**Lemmatization in NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXzuC199RVPm",
    "outputId": "20517cc4-e6ed-42fb-9089-62ee1dc22aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A paragraph group word put togeth form group usual longer sentenc .', 'paragraph often made sever sentenc .', 'there usual three eight sentenc .', 'paragraph begin indent ( five space ) , miss line , start .', 'thi make easier see one paragraph end anoth begin .', 'In organ form write , essay , paragraph contain topic sentenc .', 'thi topic sentenc paragraph tell reader paragraph .', 'essay usual multipl paragraph make claim support thesi statement , central idea essay .', 'paragraph may signal writer chang topic .', 'each paragraph may number sentenc , depend topic .']\n"
     ]
    }
   ],
   "source": [
    "#lemmatization in NLP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences_1 = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(sentences_1)):\n",
    "  word = nltk.word_tokenize(sentences_1[i])\n",
    "  word = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "  sentences_1[i] = ' '.join(words)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mguf_pF81-fX"
   },
   "source": [
    "**Implementation Of Bag Of Words(BOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1aP0n2A4tCI6"
   },
   "outputs": [],
   "source": [
    "#implementation of Bag of Words(BOW)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8eBnmQMGyHLM"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "#These steps can be considered as light text pre-processing\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabeticals \n",
    "  review = review.lower() #lower-casing the words\n",
    "  review = review.split() #removind the spaces before and after words\n",
    "  review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n",
    "  review = ' '.join(review) #joining each words separated by a space in between\n",
    "  corpus.append(review) #add above item to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M2qVU0Lj2I7C"
   },
   "outputs": [],
   "source": [
    "#creating bag of word(bow) models\n",
    "from sklearn.feature_extraction.text import CountVectorizer #importing for creating bow model \n",
    "\n",
    "count_vector = CountVectorizer() #initialization\n",
    "bow = count_vector.fit_transform(corpus).toarray() #creating bow vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc0dUMPoAfSA"
   },
   "source": [
    "**Implementation of Term Frequency-Inverse Document Frequency (TF-IDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "EIzsO05Y2iXS",
    "outputId": "09445005-1ceb-48ee-d838-0afb32f5f61c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsentences = nltk.sent_tokenize(paragraph)\\n\\ncorpus = []\\n\\n#These steps can be considered as light text pre-processing\\nfor i in range(len(sentences)):\\n  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabeticals \\n  review = review.lower() #lower-casing the words\\n  review = review.split() #removind the spaces before and after words\\n  review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\\n  review = ' '.join(review) #joining each words separated by a space in between\\n  corpus.append(review) #add above item to a new list\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these steps are the same as above\n",
    "\"\"\"\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "#These steps can be considered as light text pre-processing\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabeticals \n",
    "  review = review.lower() #lower-casing the words\n",
    "  review = review.split() #removind the spaces before and after words\n",
    "  review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n",
    "  review = ' '.join(review) #joining each words separated by a space in between\n",
    "  corpus.append(review) #add above item to a new list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ETn4ZeueA4nd"
   },
   "outputs": [],
   "source": [
    "#creating TF-IDF Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf_vector = tf_idf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYFE7sv_SZYA"
   },
   "source": [
    "**Implementation of Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "W4fZaUtBBTjv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LGbKrnPcSh7q"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "#These steps can be considered as light text pre-processing\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabetical ones\n",
    "  review = review.lower() #lower-casing the words\n",
    "  review = review.split() #removind the spaces before and after words\n",
    "  review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n",
    "  review = ' '.join(review) #joining each words separated by a space in between\n",
    "  corpus.append(review) #add above item to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "do0vfAcbSvXS"
   },
   "outputs": [],
   "source": [
    "corpus = [nltk.word_tokenize(word) for word in corpus] #word tokenizing the sentences in the \"corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgRUf-5jTQ2G",
    "outputId": "43ce1dac-d592-4e4c-8ab4-ac6d83577e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paragraph',\n",
       "  'group',\n",
       "  'word',\n",
       "  'put',\n",
       "  'together',\n",
       "  'form',\n",
       "  'group',\n",
       "  'usually',\n",
       "  'longer',\n",
       "  'sentence'],\n",
       " ['paragraph', 'often', 'made', 'several', 'sentence'],\n",
       " ['usually', 'three', 'eight', 'sentence'],\n",
       " ['paragraph',\n",
       "  'begin',\n",
       "  'indentation',\n",
       "  'five',\n",
       "  'space',\n",
       "  'missing',\n",
       "  'line',\n",
       "  'starting'],\n",
       " ['make', 'easier', 'see', 'one', 'paragraph', 'end', 'another', 'begin'],\n",
       " ['organized',\n",
       "  'form',\n",
       "  'writing',\n",
       "  'essay',\n",
       "  'paragraph',\n",
       "  'contain',\n",
       "  'topic',\n",
       "  'sentence'],\n",
       " ['topic', 'sentence', 'paragraph', 'tell', 'reader', 'paragraph'],\n",
       " ['essay',\n",
       "  'usually',\n",
       "  'multiple',\n",
       "  'paragraph',\n",
       "  'make',\n",
       "  'claim',\n",
       "  'support',\n",
       "  'thesis',\n",
       "  'statement',\n",
       "  'central',\n",
       "  'idea',\n",
       "  'essay'],\n",
       " ['paragraph', 'may', 'signal', 'writer', 'change', 'topic'],\n",
       " ['paragraph', 'may', 'number', 'sentence', 'depending', 'topic']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bLTdG9IAS95Y"
   },
   "outputs": [],
   "source": [
    "#training the word2vec model\n",
    "model = Word2Vec(corpus, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NWks2S2dTG3j"
   },
   "outputs": [],
   "source": [
    "#finding word vectors\n",
    "vector = model.wv['thesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPMQYoObTitp",
    "outputId": "d4211d70-359f-482f-fc3d-0649be65aba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.5110135e-03, -4.0345048e-03, -4.3988540e-03, -4.6293857e-03,\n",
       "       -5.5957153e-03, -5.3018047e-03, -8.0223037e-03,  9.5188450e-03,\n",
       "        6.3990639e-03, -3.6066938e-03,  2.4784422e-03, -7.6424954e-03,\n",
       "        7.5231004e-03,  8.3047338e-03,  7.8943017e-04, -6.8329908e-03,\n",
       "       -2.9577280e-03,  4.7339676e-03, -2.9393840e-03,  3.1764721e-03,\n",
       "        9.4087152e-03,  4.3531060e-03, -5.1454809e-03,  5.4670931e-03,\n",
       "       -2.8791928e-03, -6.3939407e-03,  7.0078014e-03, -9.2539331e-03,\n",
       "       -1.1306572e-03, -1.3853407e-03, -8.4159467e-03, -1.1023998e-03,\n",
       "        5.5972575e-03, -5.2676536e-03, -7.0261359e-03,  6.2002684e-03,\n",
       "       -3.4264398e-03, -7.8195576e-03,  2.9336929e-04, -4.9803257e-05,\n",
       "        7.5409054e-03,  5.6930067e-04,  7.8824591e-03, -9.8127816e-03,\n",
       "        4.0332675e-03,  6.2181093e-03,  1.0294437e-03, -2.8364109e-03,\n",
       "        1.0266138e-03, -8.3912135e-04, -8.1082704e-03, -6.7834733e-03,\n",
       "        1.2001538e-03, -2.0178747e-03,  9.5494175e-03, -9.4942807e-04,\n",
       "        6.1186957e-03,  2.0461369e-03,  8.0996891e-03, -9.2905210e-03,\n",
       "        2.3927283e-03, -1.4512587e-03,  2.1549987e-03, -2.1980047e-04,\n",
       "       -6.6406322e-03,  9.0504549e-03, -2.8433991e-03,  1.9143128e-03,\n",
       "       -6.4416979e-03,  3.7142180e-03,  9.9958042e-03, -1.1962557e-03,\n",
       "        6.5802168e-03, -1.2432098e-04,  7.9281805e-03,  9.6509289e-03,\n",
       "       -3.9128782e-04, -1.6983438e-03, -9.5216157e-03,  9.8748729e-03,\n",
       "       -2.5329662e-03,  2.1891904e-03, -1.6988873e-03,  8.3273482e-03,\n",
       "        8.3437776e-03,  1.5368843e-03,  2.8856660e-03,  6.4340639e-03,\n",
       "        2.2477007e-03,  6.4443159e-03,  1.4673948e-04, -2.2854710e-03,\n",
       "        5.3072525e-03, -3.4152747e-03, -2.6156115e-03,  8.7917279e-03,\n",
       "       -9.1546942e-03, -3.4136439e-03, -2.9241275e-03,  5.6689288e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "kNlrb-MxTjm8"
   },
   "outputs": [],
   "source": [
    "#most similar words\n",
    "similar = model.wv.most_similar('claim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zED1AQdXTrP-",
    "outputId": "62bf7a6a-6e6e-4605-98c0-bcf0877b50ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('make', 0.28518426418304443),\n",
       " ('see', 0.2704096734523773),\n",
       " ('contain', 0.25991323590278625),\n",
       " ('sentence', 0.18917794525623322),\n",
       " ('signal', 0.14294828474521637),\n",
       " ('five', 0.10772787779569626),\n",
       " ('paragraph', 0.09967141598463058),\n",
       " ('missing', 0.09729356318712234),\n",
       " ('group', 0.0963645651936531),\n",
       " ('longer', 0.08679217845201492)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukjPSlzUgUr9"
   },
   "source": [
    "**Implementation of Parts of Speech Tagging(POS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5EN1sgoOgbQJ"
   },
   "outputs": [],
   "source": [
    "#replace newlines with spaces\n",
    "text = paragraph.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "KJwFNF01ge8-",
    "outputId": "fe25f532-f574-4729-c6ac-ba232439ec37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A paragraph is a group of words put together to form a group that is usually longer than a sentence. Paragraphs are often made up of several sentences. There are usually between three and eight sentences. Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again. This makes it easier to see when one paragraph ends and another begins.  In most organized forms of writing, such as essays, paragraphs contain a topic sentence . This topic sentence of the paragraph tells the reader what the paragraph will be about. Essays usually have multiple paragraphs that make claims to support a thesis statement, which is the central idea of the essay.  Paragraphs may signal when the writer changes topics. Each paragraph may have a number of sentences, depending on the topic.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "l1UmIHPvgr0C"
   },
   "outputs": [],
   "source": [
    "#tokenize the text into words\n",
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UjXU5o24g2_R"
   },
   "outputs": [],
   "source": [
    "#process the list of words with the NLTK parts of speech tagger\n",
    "words_with_pos = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVjzXvSRhGTT",
    "outputId": "d5c9bd7d-f312-4935-b3e9-d88dc8658dff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('paragraph', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('group', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('words', 'NNS'),\n",
       " ('put', 'VBN'),\n",
       " ('together', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('form', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('group', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('usually', 'RB'),\n",
       " ('longer', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Paragraphs', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('often', 'RB'),\n",
       " ('made', 'VBN'),\n",
       " ('up', 'IN'),\n",
       " ('of', 'IN'),\n",
       " ('several', 'JJ'),\n",
       " ('sentences', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('usually', 'RB'),\n",
       " ('between', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('and', 'CC'),\n",
       " ('eight', 'CD'),\n",
       " ('sentences', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Paragraphs', 'NNP'),\n",
       " ('can', 'MD'),\n",
       " ('begin', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('indentation', 'NN'),\n",
       " ('(', '('),\n",
       " ('about', 'IN'),\n",
       " ('five', 'CD'),\n",
       " ('spaces', 'NNS'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('by', 'IN'),\n",
       " ('missing', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('line', 'NN'),\n",
       " ('out', 'IN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('then', 'RB'),\n",
       " ('starting', 'VBG'),\n",
       " ('again', 'RB'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('makes', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('easier', 'JJR'),\n",
       " ('to', 'TO'),\n",
       " ('see', 'VB'),\n",
       " ('when', 'WRB'),\n",
       " ('one', 'CD'),\n",
       " ('paragraph', 'NN'),\n",
       " ('ends', 'VBZ'),\n",
       " ('and', 'CC'),\n",
       " ('another', 'DT'),\n",
       " ('begins', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('most', 'JJS'),\n",
       " ('organized', 'JJ'),\n",
       " ('forms', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('writing', 'VBG'),\n",
       " (',', ','),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('essays', 'NNS'),\n",
       " (',', ','),\n",
       " ('paragraphs', 'NN'),\n",
       " ('contain', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('topic', 'NN'),\n",
       " ('sentence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('topic', 'JJ'),\n",
       " ('sentence', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('paragraph', 'NN'),\n",
       " ('tells', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('reader', 'NN'),\n",
       " ('what', 'WP'),\n",
       " ('the', 'DT'),\n",
       " ('paragraph', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('about', 'RB'),\n",
       " ('.', '.'),\n",
       " ('Essays', 'NNS'),\n",
       " ('usually', 'RB'),\n",
       " ('have', 'VBP'),\n",
       " ('multiple', 'JJ'),\n",
       " ('paragraphs', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('make', 'VBP'),\n",
       " ('claims', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('support', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('thesis', 'NN'),\n",
       " ('statement', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('central', 'JJ'),\n",
       " ('idea', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('essay', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Paragraphs', 'NNP'),\n",
       " ('may', 'MD'),\n",
       " ('signal', 'VB'),\n",
       " ('when', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('writer', 'NN'),\n",
       " ('changes', 'NNS'),\n",
       " ('topics', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Each', 'DT'),\n",
       " ('paragraph', 'NN'),\n",
       " ('may', 'MD'),\n",
       " ('have', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('number', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('sentences', 'NNS'),\n",
       " (',', ','),\n",
       " ('depending', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('topic', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_with_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFihrHP8hJpc",
    "outputId": "92b7a4ff-aead-4927-c4b8-11cddbb4f02c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "#to know the POS Tag meanings\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TIT9TWKuBWn"
   },
   "source": [
    "**Implementation of N-gram model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "A51TEtdnhQVL"
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "#These steps can be considered as light text pre-processing\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replacing characters with space other than alphabeticals \n",
    "  review = review.lower() #lower-casing the words\n",
    "  review = review.split() #removind the spaces before and after words\n",
    "  review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))] #removing stop-words\n",
    "  review = ' '.join(review) #joining each words separated by a space in between\n",
    "  corpus.append(review) #add above item to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "kZkhZYLBuOwK"
   },
   "outputs": [],
   "source": [
    "#creating n-gram model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "bigram_vector = bigram_vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwFbBUm7uqR5",
    "outputId": "92dcc59d-cb79-4c47-9d27-58e46c9a3eca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another',\n",
       " 'another begin',\n",
       " 'begin',\n",
       " 'begin indentation',\n",
       " 'central',\n",
       " 'central idea',\n",
       " 'change',\n",
       " 'change topic',\n",
       " 'claim',\n",
       " 'claim support',\n",
       " 'contain',\n",
       " 'contain topic',\n",
       " 'depending',\n",
       " 'depending topic',\n",
       " 'easier',\n",
       " 'easier see',\n",
       " 'eight',\n",
       " 'eight sentence',\n",
       " 'end',\n",
       " 'end another',\n",
       " 'essay',\n",
       " 'essay paragraph',\n",
       " 'essay usually',\n",
       " 'five',\n",
       " 'five space',\n",
       " 'form',\n",
       " 'form group',\n",
       " 'form writing',\n",
       " 'group',\n",
       " 'group usually',\n",
       " 'group word',\n",
       " 'idea',\n",
       " 'idea essay',\n",
       " 'indentation',\n",
       " 'indentation five',\n",
       " 'line',\n",
       " 'line starting',\n",
       " 'longer',\n",
       " 'longer sentence',\n",
       " 'made',\n",
       " 'made several',\n",
       " 'make',\n",
       " 'make claim',\n",
       " 'make easier',\n",
       " 'may',\n",
       " 'may number',\n",
       " 'may signal',\n",
       " 'missing',\n",
       " 'missing line',\n",
       " 'multiple',\n",
       " 'multiple paragraph',\n",
       " 'number',\n",
       " 'number sentence',\n",
       " 'often',\n",
       " 'often made',\n",
       " 'one',\n",
       " 'one paragraph',\n",
       " 'organized',\n",
       " 'organized form',\n",
       " 'paragraph',\n",
       " 'paragraph begin',\n",
       " 'paragraph contain',\n",
       " 'paragraph end',\n",
       " 'paragraph group',\n",
       " 'paragraph make',\n",
       " 'paragraph may',\n",
       " 'paragraph often',\n",
       " 'paragraph tell',\n",
       " 'put',\n",
       " 'put together',\n",
       " 'reader',\n",
       " 'reader paragraph',\n",
       " 'see',\n",
       " 'see one',\n",
       " 'sentence',\n",
       " 'sentence depending',\n",
       " 'sentence paragraph',\n",
       " 'several',\n",
       " 'several sentence',\n",
       " 'signal',\n",
       " 'signal writer',\n",
       " 'space',\n",
       " 'space missing',\n",
       " 'starting',\n",
       " 'statement',\n",
       " 'statement central',\n",
       " 'support',\n",
       " 'support thesis',\n",
       " 'tell',\n",
       " 'tell reader',\n",
       " 'thesis',\n",
       " 'thesis statement',\n",
       " 'three',\n",
       " 'three eight',\n",
       " 'together',\n",
       " 'together form',\n",
       " 'topic',\n",
       " 'topic sentence',\n",
       " 'usually',\n",
       " 'usually longer',\n",
       " 'usually multiple',\n",
       " 'usually three',\n",
       " 'word',\n",
       " 'word put',\n",
       " 'writer',\n",
       " 'writer change',\n",
       " 'writing',\n",
       " 'writing essay']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to know the vocabulary that the model uses\n",
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "P3fyL9OAusVD",
    "outputId": "99197af5-7732-4617-aee3-6eb8b4728671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In similar way, we can create any kind of n-grams(uni-gram, bi-gram, \\ntri-gram and so on) just by modifying the \"ngram_range\" attribute of \"CountVectorizer()\". '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In similar way, we can create any kind of n-grams(uni-gram, bi-gram, \n",
    "tri-gram and so on) just by modifying the \"ngram_range\" attribute of \"CountVectorizer()\". \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yThJUDO6vrMf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-Basics",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
